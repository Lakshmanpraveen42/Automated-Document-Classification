{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import easyocr\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe0910",
   "metadata": {},
   "source": [
    "DATASET LINK - https://www.kaggle.com/datasets/patrickaudriaz/tobacco3482jpg?select=Tobacco3482-jpg \n",
    "\n",
    "BETTER TO RUN THIS CODE IN GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=Image.open('files/dataset165/email/doc_000042.png')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader=easyocr.Reader(['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=reader.readtext('D:\\\\real_world_projects\\\\Document_classifi\\\\Dataset\\\\Email\\\\80909413.jpg')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bbox=[i[0] for i in result]\n",
    "word=[j[1] for j in result]\n",
    "print(len(bbox))\n",
    "print(len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bbox(bbox):\n",
    "    left, top = bbox[0]\n",
    "    right, bottom = bbox[2]\n",
    "    return [int(left), int(top), int(right), int(bottom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e106aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,20))\n",
    "left_image = Image.open('D:\\\\real_world_projects\\\\Document_classifi\\\\Dataset\\\\Email\\\\80909413.jpg').convert(\"RGB\")\n",
    "right_image = Image.new(\"RGB\", left_image.size, (255, 255, 255))\n",
    "\n",
    "# Draw on images\n",
    "left_draw = ImageDraw.Draw(left_image)\n",
    "right_draw = ImageDraw.Draw(right_image)\n",
    "\n",
    "font = ImageFont.truetype(\"arial\", 30)\n",
    "\n",
    "for i, (bbox, word, confidence) in enumerate(result):\n",
    "    left, top, right, bottom = create_bbox(bbox)\n",
    "    left_draw.rectangle([left, top, right, bottom], outline=\"blue\", width=2)\n",
    "    left_draw.text((right + 5, top), text=str(i + 1),font=font, fill=\"red\")\n",
    "    right_draw.text((left, top), text=word,font=font, fill=\"black\")\n",
    "\n",
    "# Display images\n",
    "ax1.imshow(left_image)\n",
    "ax1.set_title('Original_Image')\n",
    "ax2.set_title('Extracted_text')\n",
    "ax2.imshow(right_image)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagepaths=list(Path('Dataset').glob('*/*.jpg'))\n",
    "print(\"Toatal images : \",len(imagepaths))\n",
    "imag=Image.open(imagepaths[1]).convert('RGB')\n",
    "imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1175237",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagepath in tqdm(imagepaths[:5]):\n",
    "    ocr_result=reader.readtext(str(imagepath),batch_size=16)\n",
    "\n",
    "    ocr_data=[]\n",
    "    for bbox , word , confidence in ocr_result:\n",
    "        ocr_data.append({'word':word ,\n",
    "                         'bbox':create_bbox(bbox)})\n",
    "    print(ocr_data,end='\\n\\n')\n",
    "    '''with imagepath.with_suffix('.json').open('w') as f:\n",
    "        json.dump(ocr_data,f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_Extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "processor = LayoutLMv3Processor(feature_Extractor , tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_bounding_box(box, width_scale, height_scale):\n",
    "    return [\n",
    "        int(box[0] * width_scale),\n",
    "        int(box[1] * height_scale),\n",
    "        int(box[2] * width_scale),\n",
    "        int(box[3] * height_scale)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1=Image.open(imagepath).convert('RGB')\n",
    "width , height= image1.size\n",
    "width_scale=1000/width\n",
    "height_scale=1000/height\n",
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words=[]\n",
    "boxes=[]\n",
    "for row in ocr_data:\n",
    "    words.append(row['word'])\n",
    "    boxes.append(scale_bounding_box(row['bbox'],width_scale,height_scale))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding=processor(\n",
    "    image1,\n",
    "    words,\n",
    "    boxes=boxes,\n",
    "    max_length=512,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d602cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df04ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3591b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LayoutLMv3ForSequenceClassification.from_pretrained(\"microsoft/layoutlmv3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b526d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=[p.name for p in list(Path('Dataset').glob('*'))]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a335bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document_classifi(Dataset):\n",
    "    def __init__(self,img_paths,processor):\n",
    "        self.img_paths=img_paths\n",
    "        self.processor=processor    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        img_path=self.img_paths[item]\n",
    "\n",
    "        image=Image.open(img_path).convert('RGB')\n",
    "        width , height=image.size\n",
    "        width_scale=1000/width\n",
    "        height_scale=1000/height\n",
    "        \n",
    "        json_path=img_path.with_suffix('.json')\n",
    "\n",
    "        with json_path.open('r') as f:\n",
    "            ocr_result=json.load(f)\n",
    "\n",
    "        \n",
    "        words=[]\n",
    "        boxes=[]\n",
    "        for row in ocr_result:\n",
    "            words.append(row['word'])\n",
    "            boxes.append(scale_bounding_box(row['bbox'],width_scale,height_scale))\n",
    "\n",
    "        encoding=processor(\n",
    "        image,\n",
    "        words,\n",
    "        boxes=boxes,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt')\n",
    "\n",
    "        label=classes.index(img_path.parent.name)\n",
    "\n",
    "        return dict(\n",
    "            input_ids=encoding['input_ids'].flatten(),\n",
    "            attention_mask=encoding['attention_mask'].flatten(),\n",
    "            bbox=encoding['bbox'].flatten(end_dim=1),\n",
    "            pixel_values=encoding['pixel_values'].flatten(end_dim=1),\n",
    "            labels=torch.tensor(label,dtype=torch.int)\n",
    "        )\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img,test_img=train_test_split(imagepaths,test_size=0.2)\n",
    "len(train_img) , len(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0475f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=Document_classifi(train_img,processor)\n",
    "test_dataset=Document_classifi(test_img,processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f867b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_dataset:\n",
    "    print(item['bbox'].shape)\n",
    "    print(item['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518dd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader=DataLoader(train_dataset,batch_size=4,shuffle=True,num_workers=2)\n",
    "test_data_loader=DataLoader(test_dataset,batch_size=4,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc420f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelModul(nn.Module):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.model = LayoutLMv3ForSequenceClassification.from_pretrained(\"microsoft/layoutlmv3-base\", num_labels=n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, bbox, pixel_values, labels=None):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "# Training model\n",
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, n_classes, checkpoint_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    train_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes).to(device)\n",
    "    val_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes).to(device)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_accuracy.reset()\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'bbox': batch['bbox'].to(device),\n",
    "                'pixel_values': batch['pixel_values'].to(device),\n",
    "                'labels': batch['labels'].to(device)\n",
    "            }\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                inputs['input_ids'], \n",
    "                inputs['attention_mask'], \n",
    "                inputs['bbox'], \n",
    "                inputs['pixel_values'], \n",
    "                inputs['labels']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            train_accuracy.update(preds, inputs['labels'])\n",
    "\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_train_acc = train_accuracy.compute()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_accuracy.reset()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {\n",
    "                    'input_ids': batch['input_ids'].to(device),\n",
    "                    'attention_mask': batch['attention_mask'].to(device),\n",
    "                    'bbox': batch['bbox'].to(device),\n",
    "                    'pixel_values': batch['pixel_values'].to(device),\n",
    "                    'labels': batch['labels'].to(device)\n",
    "                }\n",
    "\n",
    "                outputs = model(\n",
    "                    inputs['input_ids'], \n",
    "                    inputs['attention_mask'], \n",
    "                    inputs['bbox'], \n",
    "                    inputs['pixel_values']\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "                val_accuracy.update(preds, inputs['labels'])\n",
    "\n",
    "        epoch_val_acc = val_accuracy.compute()\n",
    "\n",
    "\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f'Checkpoint saved at epoch {epoch} with validation accuracy: {epoch_val_acc:.4f}')\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_train_acc:.4f}, Validation Accuracy: {epoch_val_acc:.4f}')\n",
    "\n",
    "    print('Training complete')\n",
    "    print(f'Best validation accuracy: {best_val_acc:.4f}')\n",
    "\n",
    "n_classes = len(classes)\n",
    "model = ModelModul(n_classes=n_classes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.00001)\n",
    "checkpoint_path = \"best_model.pth\"\n",
    "\n",
    "train_model(model, train_data_loader, test_data_loader, loss_fn, optimizer, num_epochs=5, n_classes=n_classes, checkpoint_path=checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelModule(pl.LightningModule):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/layoutlmv3-base\",\n",
    "            num_labels=n_classes\n",
    "        )\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, bbox, pixel_values, labels=None):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            batch[\"bbox\"],\n",
    "            batch[\"pixel_values\"],\n",
    "            labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        print(f\"Processing batch {batch_idx}\")\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_accuracy(outputs.logits, labels)\n",
    "        self.log(\"train_acc\", self.train_accuracy, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            batch[\"bbox\"],\n",
    "            batch[\"pixel_values\"],\n",
    "            labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx}\")\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.val_accuracy(outputs.logits, labels)\n",
    "        self.log(\"val_acc\", self.val_accuracy, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_module=ModelModule(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b390f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chechpoint = ModelCheckpoint(\n",
    "    filename=\"{epoch}-{step}-{val_loss:.4f}\",\n",
    "    save_last=True,\n",
    "    save_top_k=2,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[\n",
    "        model_chechpoint\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28bcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model_module , train_data_loader , test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_document_class(im_path , model , processor ):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    image=Image.open(im_path).convert('RGB')\n",
    "    width , height=image.size\n",
    "    width_scale=1000/width\n",
    "    height_scale=1000/height\n",
    "    \n",
    "    json_path=im_path.with_suffix('.json')\n",
    "    with json_path.open('r') as f:\n",
    "        ocr_result=json.load(f)\n",
    "\n",
    "    \n",
    "    words=[]\n",
    "    boxes=[]\n",
    "    for row in ocr_result:\n",
    "        words.append(row['word'])\n",
    "        boxes.append(scale_bounding_box(row['bbox'],width_scale,height_scale))\n",
    "\n",
    "    encoding=processor(\n",
    "    image,\n",
    "    words,\n",
    "    boxes=boxes,\n",
    "    max_length=512,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output=model(\n",
    "            input_ids=encoding['input_ids'].to(device),\n",
    "            attention_mask=encoding['attention_mask'].to(device),\n",
    "            bbox=encoding['bbox'].to(device),\n",
    "            pixel_values=encoding['pixel_values'].to(device)\n",
    "        )\n",
    "\n",
    "    predict_class=output.logits.argmax()\n",
    "    return model.config.id2label[predict_class.item()]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75551b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "im_path=''\n",
    "print('Actual Label: ',im_path.parent.name)\n",
    "print('Predicted Label: ',predict_document_class(im_path,model,processor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
